{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from array import array\n",
    "import struct\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from one_layer_nn import OneLayerNN\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_data(path):\n",
    "    data_dir = Path(\"./data\")\n",
    "    with open(data_dir / path, \"rb\") as f:\n",
    "        # IDX file format\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        image_data = array(\"B\", f.read())   \n",
    "    images = []\n",
    "    for i in range(size):\n",
    "        image = np.array(image_data[i * rows * cols:(i + 1) * rows * cols]).reshape(28, 28)\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "def read_labels(path):\n",
    "    data_dir = Path(\"./data\")\n",
    "    with open(data_dir / path, \"rb\") as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        labels = np.array(array(\"B\", f.read()))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "train_val_labels = read_labels(\"train-labels.idx1-ubyte\")\n",
    "train_val_images = read_image_data(\"train-images.idx3-ubyte\")\n",
    "test_images = read_image_data(\"t10k-images.idx3-ubyte\")\n",
    "test_labels = read_labels(\"t10k-labels.idx1-ubyte\")\n",
    "print(train_val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb136ac5120>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcEUlEQVR4nO3df3TU9b3n8dcEyACaTAwhv0qgARWsQLxSSbMojZJDiGe5oBxX/HEPeD24YHAFanXTVRHbbSzuWldvqvfusVDPFUV3BY5cSw8GE9aaYAG5XG7blOTEEi4kKN1kQpAQks/+wTrtSCJ+h5m8k8nzcc73HDLzfef74dspT7/M8I3POecEAEA/S7BeAABgaCJAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAxHDrBXxZT0+Pjh07pqSkJPl8PuvlAAA8cs6pvb1d2dnZSkjo+zpnwAXo2LFjysnJsV4GAOASNTU1ady4cX0+P+AClJSUJEm6UbdquEYYrwYA4NU5dekDvRv687wvMQtQRUWFnn32WTU3NysvL08vvviiZs6cedG5L/7abbhGaLiPAAHAoPP/7zB6sbdRYvIhhM2bN2vNmjVau3at9u/fr7y8PBUXF+vEiROxOBwAYBCKSYCee+45LVu2TPfdd5++9a1v6eWXX9bo0aP185//PBaHAwAMQlEP0NmzZ7Vv3z4VFRX9+SAJCSoqKlJNTc0F+3d2dioYDIZtAID4F/UAffbZZ+ru7lZGRkbY4xkZGWpubr5g//LycgUCgdDGJ+AAYGgw/4eoZWVlamtrC21NTU3WSwIA9IOofwouLS1Nw4YNU0tLS9jjLS0tyszMvGB/v98vv98f7WUAAAa4qF8BJSYmasaMGaqsrAw91tPTo8rKShUUFET7cACAQSom/w5ozZo1WrJkib797W9r5syZev7559XR0aH77rsvFocDAAxCMQnQnXfeqU8//VRPPvmkmpubdd1112nHjh0XfDABADB0+ZxzznoRfykYDCoQCKhQC7gTAgAMQudcl6q0TW1tbUpOTu5zP/NPwQEAhiYCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgYrj1AoBYODN/ZkRzo3653/OM+/a3PM80/vVlnmduuuVfPM/8n13TPM9EKqum2/PMyHc+isFKMFhwBQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOBmpOhXw9LGeJ7p3jzK88wbVz3neUaSWrpHeJ4JJFR5nhk/fLTnmYgs2d0/x5F04t7TnmeOvZDoeeY//vhhzzNj/meN5xnEHldAAAATBAgAYCLqAXrqqafk8/nCtilTpkT7MACAQS4m7wFde+21eu+99/58kOG81QQACBeTMgwfPlyZmZmx+NYAgDgRk/eADh8+rOzsbE2cOFH33HOPjhw50ue+nZ2dCgaDYRsAIP5FPUD5+fnauHGjduzYoZdeekmNjY266aab1N7e3uv+5eXlCgQCoS0nJyfaSwIADEBRD1BJSYnuuOMOTZ8+XcXFxXr33XfV2tqqN998s9f9y8rK1NbWFtqampqivSQAwAAU808HpKSk6Oqrr1Z9fX2vz/v9fvn9/lgvAwAwwMT83wGdOnVKDQ0NysrKivWhAACDSNQD9Mgjj6i6ulqffPKJPvzwQ912220aNmyY7rrrrmgfCgAwiEX9r+COHj2qu+66SydPntTYsWN14403qra2VmPHjo32oQAAg5jPOeesF/GXgsGgAoGACrVAw33ebwyJga3htb/yPFNX+EoMVhI9P2vN9Tyzv32855mjHSmeZyI1zNfjeeafJr8Tg5Vc6JNz3m96uvyelREdK+GDAxHNDXXnXJeqtE1tbW1KTk7ucz/uBQcAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmIj5D6RD/HIFeZ5nNv+7v4/gSN5fpjs+Hx3BcaRnvr/E80zSv37m/UCf/snzSML/7b+fFuwShnmeufq/P+h55rf/4UXPM5NGXO555vPHg55nJCmwNMPzzLnmloiONRRxBQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3A0bEesKJHqeuS7R+0uuR87zzPc3/K3nGUnK2fKh55nuiI40wPV4/11dubrW88w1iSs9zxxc8D88z1RP+1+eZyRpVpH3O3wH/pG7YX9dXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSki1j3S1y/Hmf7hUs8z4/+r95uKov9dVbrH88z2oizPM3dcftLzjCS1/nWH55nAP0Z0qCGJKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQ3I0XEJpf9a78cZ9i+pH45DgaH//KbhZ5n7rj5lYiOVXrtbs8z23VFRMcairgCAgCYIEAAABOeA7R7927Nnz9f2dnZ8vl82rp1a9jzzjk9+eSTysrK0qhRo1RUVKTDhw9Ha70AgDjhOUAdHR3Ky8tTRUVFr8+vX79eL7zwgl5++WXt2bNHl112mYqLi3XmzJlLXiwAIH54/hBCSUmJSkpKen3OOafnn39ejz/+uBYsWCBJevXVV5WRkaGtW7dq8eLFl7ZaAEDciOp7QI2NjWpublZRUVHosUAgoPz8fNXU1PQ609nZqWAwGLYBAOJfVAPU3NwsScrIyAh7PCMjI/Tcl5WXlysQCIS2nJycaC4JADBAmX8KrqysTG1tbaGtqanJekkAgH4Q1QBlZmZKklpaWsIeb2lpCT33ZX6/X8nJyWEbACD+RTVAubm5yszMVGVlZeixYDCoPXv2qKCgIJqHAgAMcp4/BXfq1CnV19eHvm5sbNSBAweUmpqq8ePHa9WqVfrRj36kq666Srm5uXriiSeUnZ2thQsXRnPdAIBBznOA9u7dq5tvvjn09Zo1ayRJS5Ys0caNG/Xoo4+qo6NDDzzwgFpbW3XjjTdqx44dGjlyZPRWDQAY9DwHqLCwUM65Pp/3+Xx6+umn9fTTT1/SwtB/EqZPiWiuMGWn55k/dHn/B8lpB7s8zyB+XVEdwX/M3nzxXdD/zD8FBwAYmggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGDC892wEX8OL0mJaG7x5Z96nrnx4N94nkl+9zeeZwAMfFwBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBkptLrknyKa+0PXGc8ziRVjIjhSQwQzAAY6roAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPcjBQR+/uTsz3PjNz+UQxWAmAw4goIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUjjzLCUgOeZpISjMVgJAHw1roAAACYIEADAhOcA7d69W/Pnz1d2drZ8Pp+2bt0a9vzSpUvl8/nCtnnz5kVrvQCAOOE5QB0dHcrLy1NFRUWf+8ybN0/Hjx8Pba+//volLRIAEH88fwihpKREJSUlX7mP3+9XZmZmxIsCAMS/mLwHVFVVpfT0dE2ePFkrVqzQyZMn+9y3s7NTwWAwbAMAxL+oB2jevHl69dVXVVlZqZ/85Ceqrq5WSUmJuru7e92/vLxcgUAgtOXk5ER7SQCAASjq/w5o8eLFoV9PmzZN06dP16RJk1RVVaU5c+ZcsH9ZWZnWrFkT+joYDBIhABgCYv4x7IkTJyotLU319fW9Pu/3+5WcnBy2AQDiX8wDdPToUZ08eVJZWVmxPhQAYBDx/Fdwp06dCruaaWxs1IEDB5SamqrU1FStW7dOixYtUmZmphoaGvToo4/qyiuvVHFxcVQXDgAY3DwHaO/evbr55ptDX3/x/s2SJUv00ksv6eDBg/rFL36h1tZWZWdna+7cufrhD38ov98fvVUDAAY9zwEqLCyUc67P53/1q19d0oJwaY7ef63nmXuS3o/oWPs7vhnRHHApOm9t67djne5J7LdjDUXcCw4AYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmov4juQHg6zp3ywzPM2/81d9FcKTIfhzMlp/M8TwTUG1ExxqKuAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwM1IAURHJjUX/9HCH55kpI7zfWPTBf5vleUaSUjbv9zzjIjrS0MQVEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggpuRxpnkT7o9z3xy7nQMVoLBzDfc+x8NravbPc/svf4NzzM7Px/leeYPT1zreUaSErv2RjSHr4crIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABDcjjTOX/e89nmd2/PCaiI41aeSnnmcOj5vqeebc0X/zPBOPem68zvNM44ORHWvRNQc8z/w43fuNRSPx40eWeJ4Z9auPYrASXCqugAAAJggQAMCEpwCVl5frhhtuUFJSktLT07Vw4ULV1dWF7XPmzBmVlpZqzJgxuvzyy7Vo0SK1tLREddEAgMHPU4Cqq6tVWlqq2tpa7dy5U11dXZo7d646OjpC+6xevVrvvPOO3nrrLVVXV+vYsWO6/fbbo75wAMDg5ulDCDt27Aj7euPGjUpPT9e+ffs0e/ZstbW16ZVXXtGmTZt0yy23SJI2bNiga665RrW1tfrOd74TvZUDAAa1S3oPqK2tTZKUmpoqSdq3b5+6urpUVFQU2mfKlCkaP368ampqev0enZ2dCgaDYRsAIP5FHKCenh6tWrVKs2bN0tSp5z9a29zcrMTERKWkpITtm5GRoebm5l6/T3l5uQKBQGjLycmJdEkAgEEk4gCVlpbq0KFDeuONS/vsf1lZmdra2kJbU1PTJX0/AMDgENE/RF25cqW2b9+u3bt3a9y4caHHMzMzdfbsWbW2toZdBbW0tCgzM7PX7+X3++X3+yNZBgBgEPN0BeSc08qVK7Vlyxbt2rVLubm5Yc/PmDFDI0aMUGVlZeixuro6HTlyRAUFBdFZMQAgLni6AiotLdWmTZu0bds2JSUlhd7XCQQCGjVqlAKBgO6//36tWbNGqampSk5O1kMPPaSCggI+AQcACOMpQC+99JIkqbCwMOzxDRs2aOnSpZKkn/70p0pISNCiRYvU2dmp4uJi/exnP4vKYgEA8cNTgJxzF91n5MiRqqioUEVFRcSLwuDwYEqj55mW7cmeZ/b+abznmXj0TO4/eJ65LrH/7je872y355m/+eh+zzOTdv3e84z3laE/cC84AIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOi/W+ViwNr43/59RHMnHt7teWbd2H/2fqBIZuKS9/+7novwPtD/fNb7zL2b/5Pnmdz/XON5hjtbxw+ugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFEr9ufcbQkrSb3Zf7Xnmua1nPM+sueKw55l4NKX6bz3PJP7L6IiONa78Q88zuYrsdYShiysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyNFxLrrGz3PvDc1yfuMrvc8E48m6oD1EoCo4goIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmPAUoPLyct1www1KSkpSenq6Fi5cqLq6urB9CgsL5fP5wrbly5dHddEAgMHPU4Cqq6tVWlqq2tpa7dy5U11dXZo7d646OjrC9lu2bJmOHz8e2tavXx/VRQMABj9PPxF1x44dYV9v3LhR6enp2rdvn2bPnh16fPTo0crMzIzOCgEAcemS3gNqa2uTJKWmpoY9/tprryktLU1Tp05VWVmZTp8+3ef36OzsVDAYDNsAAPHP0xXQX+rp6dGqVas0a9YsTZ06NfT43XffrQkTJig7O1sHDx7UY489prq6Or399tu9fp/y8nKtW7cu0mUAAAYpn3PORTK4YsUK/fKXv9QHH3ygcePG9bnfrl27NGfOHNXX12vSpEkXPN/Z2anOzs7Q18FgUDk5OSrUAg33jYhkaQAAQ+dcl6q0TW1tbUpOTu5zv4iugFauXKnt27dr9+7dXxkfScrPz5ekPgPk9/vl9/sjWQYAYBDzFCDnnB566CFt2bJFVVVVys3NvejMgQMHJElZWVkRLRAAEJ88Bai0tFSbNm3Stm3blJSUpObmZklSIBDQqFGj1NDQoE2bNunWW2/VmDFjdPDgQa1evVqzZ8/W9OnTY/IbAAAMTp7eA/L5fL0+vmHDBi1dulRNTU269957dejQIXV0dCgnJ0e33XabHn/88a/8e8C/FAwGFQgEeA8IAAapmLwHdLFW5eTkqLq62su3BAAMUdwLDgBgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgYrj1Ar7MOSdJOqcuyRkvBgDg2Tl1Sfrzn+d9GXABam9vlyR9oHeNVwIAuBTt7e0KBAJ9Pu9zF0tUP+vp6dGxY8eUlJQkn88X9lwwGFROTo6ampqUnJxstEJ7nIfzOA/ncR7O4zycNxDOg3NO7e3tys7OVkJC3+/0DLgroISEBI0bN+4r90lOTh7SL7AvcB7O4zycx3k4j/NwnvV5+Korny/wIQQAgAkCBAAwMagC5Pf7tXbtWvn9fuulmOI8nMd5OI/zcB7n4bzBdB4G3IcQAABDw6C6AgIAxA8CBAAwQYAAACYIEADAxKAJUEVFhb75zW9q5MiRys/P10cffWS9pH731FNPyefzhW1TpkyxXlbM7d69W/Pnz1d2drZ8Pp+2bt0a9rxzTk8++aSysrI0atQoFRUV6fDhwzaLjaGLnYelS5de8PqYN2+ezWJjpLy8XDfccIOSkpKUnp6uhQsXqq6uLmyfM2fOqLS0VGPGjNHll1+uRYsWqaWlxWjFsfF1zkNhYeEFr4fly5cbrbh3gyJAmzdv1po1a7R27Vrt379feXl5Ki4u1okTJ6yX1u+uvfZaHT9+PLR98MEH1kuKuY6ODuXl5amioqLX59evX68XXnhBL7/8svbs2aPLLrtMxcXFOnPmTD+vNLYudh4kad68eWGvj9dff70fVxh71dXVKi0tVW1trXbu3Kmuri7NnTtXHR0doX1Wr16td955R2+99Zaqq6t17Ngx3X777Yarjr6vcx4kadmyZWGvh/Xr1xutuA9uEJg5c6YrLS0Nfd3d3e2ys7NdeXm54ar639q1a11eXp71MkxJclu2bAl93dPT4zIzM92zzz4beqy1tdX5/X73+uuvG6ywf3z5PDjn3JIlS9yCBQtM1mPlxIkTTpKrrq52zp3/337EiBHurbfeCu3zu9/9zklyNTU1VsuMuS+fB+ec++53v+sefvhhu0V9DQP+Cujs2bPat2+fioqKQo8lJCSoqKhINTU1hiuzcfjwYWVnZ2vixIm65557dOTIEeslmWpsbFRzc3PY6yMQCCg/P39Ivj6qqqqUnp6uyZMna8WKFTp58qT1kmKqra1NkpSamipJ2rdvn7q6usJeD1OmTNH48ePj+vXw5fPwhddee01paWmaOnWqysrKdPr0aYvl9WnA3Yz0yz777DN1d3crIyMj7PGMjAz9/ve/N1qVjfz8fG3cuFGTJ0/W8ePHtW7dOt100006dOiQkpKSrJdnorm5WZJ6fX188dxQMW/ePN1+++3Kzc1VQ0ODfvCDH6ikpEQ1NTUaNmyY9fKirqenR6tWrdKsWbM0depUSedfD4mJiUpJSQnbN55fD72dB0m6++67NWHCBGVnZ+vgwYN67LHHVFdXp7fffttwteEGfIDwZyUlJaFfT58+Xfn5+ZowYYLefPNN3X///YYrw0CwePHi0K+nTZum6dOna9KkSaqqqtKcOXMMVxYbpaWlOnTo0JB4H/Sr9HUeHnjggdCvp02bpqysLM2ZM0cNDQ2aNGlSfy+zVwP+r+DS0tI0bNiwCz7F0tLSoszMTKNVDQwpKSm6+uqrVV9fb70UM1+8Bnh9XGjixIlKS0uLy9fHypUrtX37dr3//vthP74lMzNTZ8+eVWtra9j+8fp66Os89CY/P1+SBtTrYcAHKDExUTNmzFBlZWXosZ6eHlVWVqqgoMBwZfZOnTqlhoYGZWVlWS/FTG5urjIzM8NeH8FgUHv27Bnyr4+jR4/q5MmTcfX6cM5p5cqV2rJli3bt2qXc3Nyw52fMmKERI0aEvR7q6up05MiRuHo9XOw89ObAgQOSNLBeD9afgvg63njjDef3+93GjRvdb3/7W/fAAw+4lJQU19zcbL20fvW9733PVVVVucbGRvfrX//aFRUVubS0NHfixAnrpcVUe3u7+/jjj93HH3/sJLnnnnvOffzxx+6Pf/yjc865Z555xqWkpLht27a5gwcPugULFrjc3Fz3+eefG688ur7qPLS3t7tHHnnE1dTUuMbGRvfee++566+/3l111VXuzJkz1kuPmhUrVrhAIOCqqqrc8ePHQ9vp06dD+yxfvtyNHz/e7dq1y+3du9cVFBS4goICw1VH38XOQ319vXv66afd3r17XWNjo9u2bZubOHGimz17tvHKww2KADnn3IsvvujGjx/vEhMT3cyZM11tba31kvrdnXfe6bKyslxiYqL7xje+4e68805XX19vvayYe//9952kC7YlS5Y4585/FPuJJ55wGRkZzu/3uzlz5ri6ujrbRcfAV52H06dPu7lz57qxY8e6ESNGuAkTJrhly5bF3X+k9fb7l+Q2bNgQ2ufzzz93Dz74oLviiivc6NGj3W233eaOHz9ut+gYuNh5OHLkiJs9e7ZLTU11fr/fXXnlle773/++a2trs134l/DjGAAAJgb8e0AAgPhEgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJj4f41+uUXCIXZpAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show image\n",
    "\n",
    "plt.imshow(test_images[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(label):\n",
    "    encoded_label = np.zeros(10)\n",
    "    encoded_label[label] = 1\n",
    "    return encoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set trainning dataset\n",
    "# Set validate dataset\n",
    "\n",
    "train_images = train_val_images[:int(0.9 * len(train_val_images))]\n",
    "val_images = train_val_images[len(train_images):]\n",
    "\n",
    "train_labels = train_val_labels[:len(train_images)]\n",
    "val_labels = train_val_labels[len(train_images):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer_nn = OneLayerNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33243/4021182234.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  torch_train_labels = torch.Tensor([onehot(label) for label in train_labels])\n"
     ]
    }
   ],
   "source": [
    "torch_train_images = torch.Tensor(train_images).reshape(-1, 28 * 28)\n",
    "torch_val_images = torch.Tensor(val_images).reshape(-1, 28 * 28)\n",
    "torch_test_images = torch.Tensor(test_images).reshape(-1, 28 * 28)\n",
    "torch_train_labels = torch.Tensor([onehot(label) for label in train_labels])\n",
    "torch_val_labels = torch.Tensor([onehot(label) for label in val_labels])\n",
    "torch_test_labels = torch.Tensor([onehot(label) for label in test_labels])\n",
    "torch_train_dataset = torch.hstack((torch_train_labels, torch_train_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54000, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_train_images.shape\n",
    "torch_train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(params=one_layer_nn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss 0.20393569767475128\n",
      "Accuracy 0.948\n",
      "Val Loss 0.19235552847385406\n",
      "Accuracy 0.9508333333333333\n",
      "Val Loss 0.17994824051856995\n",
      "Accuracy 0.9648333333333333\n",
      "Val Loss 0.17091968655586243\n",
      "Accuracy 0.9633333333333334\n",
      "Val Loss 0.1671697199344635\n",
      "Accuracy 0.962\n",
      "Val Loss 0.16545654833316803\n",
      "Accuracy 0.9648333333333333\n",
      "Val Loss 0.1630193591117859\n",
      "Accuracy 0.9658333333333333\n",
      "Val Loss 0.16405142843723297\n",
      "Accuracy 0.9633333333333334\n",
      "Val Loss 0.16647011041641235\n",
      "Accuracy 0.9646666666666667\n",
      "Val Loss 0.1667189747095108\n",
      "Accuracy 0.9675\n",
      "Val Loss 0.1657988429069519\n",
      "Accuracy 0.9678333333333333\n",
      "Val Loss 0.16974537074565887\n",
      "Accuracy 0.9615\n",
      "Val Loss 0.17027665674686432\n",
      "Accuracy 0.9661666666666666\n",
      "Val Loss 0.170095294713974\n",
      "Accuracy 0.9698333333333333\n",
      "Val Loss 0.17230236530303955\n",
      "Accuracy 0.9675\n",
      "Val Loss 0.1735967993736267\n",
      "Accuracy 0.969\n",
      "Val Loss 0.1737612783908844\n",
      "Accuracy 0.9718333333333333\n",
      "Val Loss 0.17466598749160767\n",
      "Accuracy 0.9716666666666667\n",
      "Val Loss 0.17464259266853333\n",
      "Accuracy 0.9721666666666666\n",
      "Val Loss 0.1760926991701126\n",
      "Accuracy 0.9703333333333334\n",
      "Val Loss 0.17875811457633972\n",
      "Accuracy 0.9701666666666666\n",
      "Val Loss 0.18208128213882446\n",
      "Accuracy 0.9665\n",
      "Val Loss 0.18383045494556427\n",
      "Accuracy 0.9713333333333334\n",
      "Val Loss 0.18647657334804535\n",
      "Accuracy 0.9671666666666666\n",
      "Val Loss 0.18809612095355988\n",
      "Accuracy 0.973\n",
      "Val Loss 0.1916925460100174\n",
      "Accuracy 0.9678333333333333\n",
      "Val Loss 0.19532498717308044\n",
      "Accuracy 0.9666666666666667\n",
      "Val Loss 0.19783997535705566\n",
      "Accuracy 0.9688333333333333\n",
      "Val Loss 0.20059695839881897\n",
      "Accuracy 0.9738333333333333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/trhoan/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/cnn_model/mnist.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/trhoan/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/cnn_model/mnist.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad() \u001b[39m# Chuyen grad ve 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/trhoan/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/cnn_model/mnist.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward() \u001b[39m# Backward \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/trhoan/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/cnn_model/mnist.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     optim\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/trhoan/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/cnn_model/mnist.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/trhoan/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/cnn_model/mnist.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/venv/lib/python3.10/site-packages/torch/optim/sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m has_sparse_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 75\u001b[0m sgd(params_with_grad,\n\u001b[1;32m     76\u001b[0m     d_p_list,\n\u001b[1;32m     77\u001b[0m     momentum_buffer_list,\n\u001b[1;32m     78\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     79\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     80\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     81\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     82\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     83\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     84\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m     85\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     87\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/venv/lib/python3.10/site-packages/torch/optim/sgd.py:220\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 220\u001b[0m func(params,\n\u001b[1;32m    221\u001b[0m      d_p_list,\n\u001b[1;32m    222\u001b[0m      momentum_buffer_list,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m    225\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    226\u001b[0m      dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[1;32m    227\u001b[0m      nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[1;32m    228\u001b[0m      has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m    229\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize)\n",
      "File \u001b[0;32m~/Documents/HUST-Documents/ML_DM/RealTime-DigitRecognition/venv/lib/python3.10/site-packages/torch/optim/sgd.py:263\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m         d_p \u001b[39m=\u001b[39m buf\n\u001b[0;32m--> 263\u001b[0m param\u001b[39m.\u001b[39;49madd_(d_p, alpha\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mlr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "one_layer_nn = OneLayerNN()\n",
    "optim = torch.optim.SGD(params=one_layer_nn.parameters(), lr=1e-3)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs = 30\n",
    "batch_size = 10\n",
    "val_predictions = []\n",
    "for epoch in range(epochs):\n",
    "    shuffled_order = torch.randperm(len(torch_train_dataset))\n",
    "    val_predictions = []\n",
    "    for index in range(len(shuffled_order) // batch_size):\n",
    "        batch = torch_train_dataset[shuffled_order[index * batch_size:(index + 1) * batch_size]]\n",
    "        y_true = batch[:,:10]\n",
    "        y_pred = one_layer_nn(batch[:,10:]) # What's happen?\n",
    "        loss = torch.nn.functional.cross_entropy(y_pred, y_true) # What's happen?\n",
    "        optim.zero_grad() # Chuyen grad ve 0\n",
    "        loss.backward() # Backward \n",
    "        optim.step()\n",
    "        train_losses.append(loss.detach())\n",
    "    with torch.no_grad():\n",
    "        for batch in range(len(torch_val_images) // batch_size):\n",
    "            y_pred = one_layer_nn(torch_val_images[batch * batch_size:(batch + 1) * batch_size])\n",
    "            y_true = torch_val_labels[batch * batch_size:(batch + 1) * batch_size]\n",
    "            loss = torch.nn.functional.cross_entropy(y_pred, y_true)\n",
    "            val_losses.append(loss)\n",
    "            val_predictions.extend(torch.argmax(y_pred, axis=1))\n",
    "        print(\"Val Loss\", (sum(val_losses) / len(val_losses)).item())\n",
    "        print(\"Accuracy\", sum(np.array(val_predictions) == val_labels) / len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(one_layer_nn.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneLayerNN()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "def convert_img_to_mnist(path):\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    image = image.convert('L')\n",
    "\n",
    "    # Resize the image to 28x28 pixels\n",
    "    image = image.resize((28, 28))\n",
    "\n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "    # Ensure pixel values are within the range [0, 255]\n",
    "    image_array = (image_array / 255.0) * 255\n",
    "    image_array = torch.Tensor(image_array).reshape(-1, 28 * 28)\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        y_pred = one_layer_nn(image_array[:])\n",
    "        test_predictions.extend(torch.argmax(y_pred, axis=1))\n",
    "        print(test_predictions)\n",
    "    test_predictions = torch.Tensor(test_predictions).numpy().astype(np.uint8)\n",
    "    print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneLayerNN()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "def convert_img_to_mnist_2(path):\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    image = image.convert('L')\n",
    "\n",
    "    # Resize the image to 28x28 pixels\n",
    "    image = image.resize((28, 28))\n",
    "\n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "    # Ensure pixel values are within the range [0, 255]\n",
    "    image_array = (image_array / 255.0) * 255\n",
    "    image_array = torch.Tensor(image_array).reshape(-1, 28 * 28)\n",
    "    test_predictions = []\n",
    "    y_pred = one_layer_nn(image_array[:])\n",
    "    max_index = np.argmax(y_pred.detach().numpy())\n",
    "    print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "image_array = convert_img_to_mnist_2('out.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
